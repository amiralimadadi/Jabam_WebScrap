{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Jabama](https://www.jabama.com/) website data scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I try to scrap data from [Jabama](https://www.jabama.com/) website. \n",
    "\n",
    "This site is providing services in the field of accommodation and is one the leaders accommodation market in Iran. There are residences available in almost all major or tourist towns in Iran. People can rent any kind of residences in the website for a short to medium time period. There are many kinds of residenc like apartment, villa, suite, complex, inn, hostel, ecotourism and etc, available in the site. \n",
    "\n",
    "Therefore we can find valuable information about accommodation conditions (like price and ...) in the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "We use 3 main libraries for web scraping:\n",
    "\n",
    "1.   Csv (for read input data and write final data)\n",
    "2.   Requests (for http request to web pages)\n",
    "3.   BeautifulSoup (the main library for web scrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software structure\n",
    "I need all accommadations information for a good analysis, but unfortunately the site does not have the feature for getting all information. I even called the website support team and they informed me that I cannot get all information from the site directly. So I did the following activities to gather a good and complete set of accommodation data set.\n",
    "\n",
    "At first, I open and read a text file called *Cities.txt*, which consists of the provinces and major cities of Iran. For all rows in the file, I build a Url for searching in the website.The Url looks like :*https://www.jabama.com/search?q=مازندران&kind=accommodation&page-number=1*, which means page 1 of the search result of all accommodation types for province or city *مازندران*. So if I search all pages (max=70) for all cities or provinces, almost the information of all residences of the site will be gathered.\n",
    "\n",
    "In each page, there will be at most 12 accommodations (at the end of year 1399). I get accommodations by web scraping and for each accommodation in the page, I build a Url to request the accommodation page. The Url looks like *https://www.jabama.com/stay/cottage-102172*. \n",
    "\n",
    "At last, I can get all information about accommodations like price, foundation, rooms and ... by scraping the page. Ofcourse some information is in the accommodation Url like accommodation kind and code. \n",
    "\n",
    "By adding new cities to *Cities.txt*, you can gather new records of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap_and_Save function\n",
    "This function get each search page result and scrap it and save the accommodation data into result file in csv format. I separate the scrap section of my code into a funtion for higher understandability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap the page soup in paramters\n",
    "# It is working based on the https://www.jabama.com/city-tehran?page-number=3 pages\n",
    "def Scrap_and_Save(soup, data_file) :\n",
    "    # Find all stays\n",
    "    stays = soup.find_all('a', attrs= {'class':'vertical-card'})\n",
    "    for each_stay in stays:\n",
    "        temp_Url = Jabama_Url_WithoutSlash + each_stay['href']\n",
    "        \n",
    "        # Check if stay is found\n",
    "        response = requests.get(temp_Url)\n",
    "        response.encoding = 'utf-8'\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find url features\n",
    "        code = kind = ''\n",
    "        split_url = each_stay['href'].split('-')\n",
    "\n",
    "        print('        scraping stay :',each_stay['href'])\n",
    "\n",
    "        if len(split_url) == 2 :\n",
    "            code = split_url[1]\n",
    "            split_url = split_url[0].split('/')\n",
    "            if len(split_url) == 3 :\n",
    "                kind = split_url[2]\n",
    "\n",
    "        # Find direct features\n",
    "        price = comment = score = city = ''\n",
    "        price = soup.find('span', attrs= {'class':'box-title'})\n",
    "        comment = soup.find('span', attrs= {'class':'count'})\n",
    "        score = soup.find('span', attrs= {'class':'score'})\n",
    "        city = soup.find('span', attrs= {'class':'city-province'})\n",
    "\n",
    "        if comment != None : comment = comment.get_text().strip().replace('(','').replace(')','')\n",
    "        if score != None : score = score.get_text().strip()\n",
    "        if city != None : city = city.get_text().strip().replace('،','-')\n",
    "        if price != None :  price = price.get_text().strip().split('تومان')[0].replace(',','')\n",
    "        # if comment == 'جدید' : comment = ''\n",
    "        # if score == '' : score = '0'\n",
    "\n",
    "       \n",
    "        # Find acommodation specification features\n",
    "        foundation = area = room = capacity = ''\n",
    "        double_bed = single_bed = iranian_bed = ''\n",
    "        toile = bath = ''\n",
    "        specifications = soup.find('div', attrs= {'class':'accommodation-info__specifications'})\n",
    "        if specifications != None:\n",
    "            specs = specifications.find_all('div', attrs= {'class':'accommodation-spec'}, recursive=False)\n",
    "            if specs != None :\n",
    "                # Count the specifications and caption row items\n",
    "                index1 = index2 = 0\n",
    "                for each_spec in specs:\n",
    "                    content = each_spec.find('div', attrs= {'class':'content'}, recursive=False)\n",
    "                    if content == None : continue\n",
    "                    caption_container = content.find('div', attrs= {'class':'caption-container'}, recursive=False)\n",
    "                    if caption_container == None : continue\n",
    "                    caption_rows = caption_container.find_all('div', attrs= {'class':'caption-row'}, recursive=False)\n",
    "                    if caption_rows == None : continue\n",
    "                    index2 = 0\n",
    "                    for each_caption_row in caption_rows:\n",
    "                        capt = each_caption_row.find('span', attrs= {'class':'caption'}, recursive=False)\n",
    "                        if capt == None : continue\n",
    "                        temp_str = capt.get_text().strip()\n",
    "                        \n",
    "                        # Specify the variable\n",
    "                        if index1 == 0:\n",
    "                            if index2 == 0: foundation = temp_str\n",
    "                            if index2 == 1: area = temp_str\n",
    "                            if index2 == 2: room = temp_str\n",
    "                        if index1 == 1:\n",
    "                            if index2 == 0: capacity = temp_str\n",
    "                        if index1 == 2:\n",
    "                            if index2 == 0: double_bed = temp_str\n",
    "                            if index2 == 1: single_bed = temp_str\n",
    "                            if index2 == 2: iranian_bed = temp_str\n",
    "                        if index1 == 3:\n",
    "                            if index2 == 0: toile = temp_str\n",
    "                            if index2 == 1: bath = temp_str\n",
    "                        index2 += 1\n",
    "                    index1 += 1\n",
    "        \n",
    "        # Find acommodation amenities features\n",
    "        water = water_cooler = refrigerator = closet = 'False'\n",
    "        cooking = oven = furniture = dining_table = 'False'\n",
    "        restaurant = green_space = lobby = elavator = 'False'\n",
    "        amenities_container = soup.find('div', attrs= {'class':'accommodation-amenities__list'})\n",
    "        if amenities_container != None :\n",
    "            amenities_list = amenities_container.find_all('div', attrs= {'class':'accommodation-amenities__amenity'}, recursive=False)\n",
    "            amenities_list_missed = amenities_container.find_all('div', attrs= {'class':'accommodation-amenities__amenity missed'}, recursive=False)\n",
    "\n",
    "            for each_aminity in amenities_list:\n",
    "                if each_aminity in amenities_list_missed : continue\n",
    "                temp_str = each_aminity.find('span', recursive=False)\n",
    "                if temp_str != None:\n",
    "                    temp_str = temp_str.get_text().strip()\n",
    "\n",
    "                    if temp_str == 'آب' : water = 'True'\n",
    "                    if temp_str == 'یخچال' : refrigerator = 'True'\n",
    "                    if temp_str == 'کولر آبی' : water_cooler = 'True'\n",
    "                    if temp_str == 'کمد/دراور' : closet = 'True'\n",
    "                    if temp_str == 'لوازم آشپزی' : cooking = 'True'\n",
    "                    if temp_str == 'اجاق گاز' : oven = 'True'\n",
    "                    if temp_str == 'مبلمان' : furniture = 'True'\n",
    "                    if temp_str == 'میز نهارخوری' : dining_table = 'True'\n",
    "                    if temp_str == 'رستوران' : restaurant = 'True'\n",
    "                    if temp_str == 'فضای سبز' : green_space = 'True'\n",
    "                    if temp_str == 'لابی' : lobby = 'True'\n",
    "                    if temp_str == 'آسانسور' : elavator = 'True'\n",
    "        \n",
    "        new_row = [code,kind,price,comment,score,city,foundation,area\n",
    "                    ,room,capacity,double_bed,single_bed,iranian_bed\n",
    "                    ,toile,bath,water,water_cooler,refrigerator,closet\n",
    "                    ,cooking,oven,furniture,dining_table,restaurant\n",
    "                    ,green_space,lobby,elavator]\n",
    "        \n",
    "        with open(data_file, 'a+', newline='', encoding='utf-8') as write_obj:\n",
    "            # Create a writer object from csv module\n",
    "            csv_writer = csv.writer(write_obj)\n",
    "            # Add contents of list as last row in the csv file\n",
    "            csv_writer.writerow(new_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function\n",
    "In the main function, I just open the *cities.txt* file, create the search Url and request the Url. Then call the Scrap_and_Save function with the Url result.\n",
    "\n",
    "Tip1 : The site returns a web page instead of not found error (404). So if the searched city was not found, the site returns a web page. Therefore, I continue the for loop by detecting a *div* with specific *class*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping اردبیل page 1 :\n",
      "        scraping stay : /stay/inn-85754\n",
      "        scraping stay : /stay/inn-85779\n",
      "        scraping stay : /stay/inn-85775\n",
      "        scraping stay : /stay/ecotourism-331103\n",
      "        scraping stay : /stay/ecotourism-331102\n",
      "        scraping stay : /stay/ecotourism-329207\n",
      "        scraping stay : /stay/ecotourism-329206\n",
      "        scraping stay : /stay/complex-93549\n",
      "        scraping stay : /stay/apartment-108984\n",
      "        scraping stay : /stay/ecotourism-88391\n",
      "        scraping stay : /stay/ecotourism-88376\n",
      "        scraping stay : /stay/apartment-332256\n",
      "Scraping اردبیل page 2 :\n",
      "        scraping stay : /stay/complex-93564\n",
      "        scraping stay : /stay/complex-93545\n",
      "        scraping stay : /stay/suite-331660\n",
      "        scraping stay : /stay/complex-93552\n",
      "        scraping stay : /stay/complex-87617\n",
      "        scraping stay : /stay/ecotourism-329208\n",
      "        scraping stay : /stay/apartment-333602\n",
      "        scraping stay : /stay/complex-93554\n",
      "        scraping stay : /stay/ecotourism-88395\n",
      "        scraping stay : /stay/villa-105371\n",
      "        scraping stay : /stay/complex-87632\n",
      "Scraping اردبیل page 3 :\n",
      "Scraping تبریز page 1 :\n",
      "        scraping stay : /stay/suite-109072\n",
      "        scraping stay : /stay/cottage-103709\n",
      "        scraping stay : /stay/suite-106114\n",
      "        scraping stay : /stay/traditional-333706\n",
      "        scraping stay : /stay/traditional-333700\n",
      "Scraping تبریز page 2 :\n",
      "Scraping ارومیه page 1 :\n",
      "        scraping stay : /stay/suite-104503\n",
      "Scraping ارومیه page 2 :\n",
      "Scraping اصفهان page 1 :\n",
      "        scraping stay : /stay/traditional-326620\n",
      "        scraping stay : /stay/ecotourism-92866\n",
      "        scraping stay : /stay/ecotourism-92879\n",
      "        scraping stay : /stay/traditional-326626\n",
      "        scraping stay : /stay/traditional-326623\n",
      "        scraping stay : /stay/traditional-326621\n",
      "        scraping stay : /stay/traditional-326624\n",
      "        scraping stay : /stay/traditional-326622\n",
      "        scraping stay : /stay/ecotourism-92878\n",
      "        scraping stay : /stay/traditional-326627\n",
      "        scraping stay : /stay/hostel-339392\n",
      "        scraping stay : /stay/hostel-336540\n",
      "Scraping اصفهان page 2 :\n",
      "        scraping stay : /stay/hostel-336535\n",
      "        scraping stay : /stay/hostel-339396\n",
      "        scraping stay : /stay/hostel-336538\n",
      "        scraping stay : /stay/hostel-339394\n",
      "        scraping stay : /stay/hostel-339391\n",
      "        scraping stay : /stay/hostel-336536\n",
      "        scraping stay : /stay/hostel-336496\n",
      "        scraping stay : /stay/hostel-336531\n",
      "        scraping stay : /stay/hostel-336532\n",
      "        scraping stay : /stay/hostel-339395\n",
      "        scraping stay : /stay/apartment-337091\n",
      "        scraping stay : /stay/traditional-326116\n",
      "Scraping اصفهان page 3 :\n",
      "        scraping stay : /stay/apartment-337091\n",
      "        scraping stay : /stay/traditional-326116\n",
      "        scraping stay : /stay/traditional-85590\n",
      "        scraping stay : /stay/apartment-334630\n",
      "        scraping stay : /stay/traditional-328101\n",
      "        scraping stay : /stay/ecotourism-93372\n",
      "        scraping stay : /stay/traditional-328106\n",
      "        scraping stay : /stay/traditional-328100\n",
      "        scraping stay : /stay/hostel-339397\n",
      "        scraping stay : /stay/apartment-102444\n",
      "        scraping stay : /stay/ecotourism-85207\n",
      "Scraping اصفهان page 4 :\n",
      "        scraping stay : /stay/inn-84094\n",
      "        scraping stay : /stay/traditional-326625\n",
      "        scraping stay : /stay/inn-87352\n",
      "        scraping stay : /stay/inn-87353\n",
      "        scraping stay : /stay/inn-84096\n",
      "        scraping stay : /stay/inn-87350\n",
      "        scraping stay : /stay/traditional-91487\n",
      "        scraping stay : /stay/traditional-91477\n",
      "        scraping stay : /stay/traditional-91475\n",
      "        scraping stay : /stay/traditional-327997\n",
      "        scraping stay : /stay/traditional-328008\n",
      "        scraping stay : /stay/traditional-328009\n",
      "Scraping اصفهان page 5 :\n",
      "        scraping stay : /stay/traditional-91488\n",
      "        scraping stay : /stay/traditional-91471\n",
      "        scraping stay : /stay/traditional-327995\n",
      "        scraping stay : /stay/traditional-328007\n",
      "        scraping stay : /stay/inn-85749\n",
      "        scraping stay : /stay/hostel-336539\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Data_File_Name = 'Data.csv'\n",
    "    City_File_Name = 'Cities.txt'\n",
    "    \n",
    "    # Write the headers in data csv file\n",
    "    with open(Data_File_Name, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        handle = csv.writer(csv_file)\n",
    "        handle.writerow(['code','kind','price','comment','score','city'\n",
    "            ,'foundation','area','room','capacity','double_bed','single_bed','iranian_bed'\n",
    "            ,'toile','bath','water','water_cooler','refrigerator','closet','cooking','oven'\n",
    "            ,'furniture','dining_table','restaurant','green_space','lobby','elavator'])\n",
    "\n",
    "    city_list = list()\n",
    "    # Read city source file\n",
    "    with open(City_File_Name,'r',encoding='utf-8') as city_file:\n",
    "        lines = city_file.readlines()\n",
    "        for each_line in lines:\n",
    "            city_list.append(each_line.replace('\\n',''))\n",
    "\n",
    "    # Get HTML of all cities\n",
    "    Jabama_Url = 'https://www.jabama.com/'\n",
    "    Jabama_Url_WithoutSlash = 'https://www.jabama.com'\n",
    "\n",
    "    for each_city in city_list :\n",
    "        # Find all pages (At last Max_page_no pages)\n",
    "        Max_page_no = 70\n",
    "        for each_page in range(Max_page_no):\n",
    "            temp_page = str(each_page + 1)\n",
    "\n",
    "            print('Scraping', each_city, 'page', temp_page, ':')\n",
    "            \n",
    "            temp_Url = f'{Jabama_Url}search?q={each_city}&kind=accommodation&page-number={temp_page}'\n",
    "\n",
    "            # Check if page is found\n",
    "            response = requests.get(temp_Url)\n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Jabama return this page instead of 404\n",
    "            check_empty = soup.find_all('div', attrs= {'class':'listing-empty-state'})\n",
    "            if len(check_empty) > 0:\n",
    "                break\n",
    "            \n",
    "            Scrap_and_Save(soup, Data_File_Name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will have a csv file called *Data.csv* after running the program. The file contains 27 features of accommodations. This file w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
